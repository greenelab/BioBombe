	loss	val_loss	num_components	learning_rate	batch_size	epochs	sparsity	noise	seed
0	0.036801776900281184	0.02664417155215207	25	0.001	100	50	0.0	0.1	7030
1	0.023058911831985177	0.021260315281682672	25	0.001	100	50	0.0	0.1	7030
2	0.01969367268491403	0.018775575414983765	25	0.001	100	50	0.0	0.1	7030
3	0.017914727257585836	0.017468544389219173	25	0.001	100	50	0.0	0.1	7030
4	0.01704132364437334	0.01685435348947178	25	0.001	100	50	0.0	0.1	7030
5	0.016666670732356868	0.01666244170156594	25	0.001	100	50	0.0	0.1	7030
6	0.01650126627322358	0.016500719381321798	25	0.001	100	50	0.0	0.1	7030
7	0.016408260823052835	0.0164262193662914	25	0.001	100	50	0.0	0.1	7030
8	0.016343263948203072	0.016430867857248104	25	0.001	100	50	0.0	0.1	7030
9	0.01631043136771264	0.01633050578359664	25	0.001	100	50	0.0	0.1	7030
10	0.016263061385409023	0.01630047385725077	25	0.001	100	50	0.0	0.1	7030
11	0.01623095305163394	0.016279839301556635	25	0.001	100	50	0.0	0.1	7030
12	0.016211119569971812	0.016250142272041815	25	0.001	100	50	0.0	0.1	7030
13	0.016190724865292296	0.01621917791016414	25	0.001	100	50	0.0	0.1	7030
14	0.01617896538283203	0.016205838357897948	25	0.001	100	50	0.0	0.1	7030
15	0.016169561254936862	0.016190181822866828	25	0.001	100	50	0.0	0.1	7030
16	0.016149686294642744	0.016178029352517703	25	0.001	100	50	0.0	0.1	7030
17	0.016153916811755938	0.016212092051870165	25	0.001	100	50	0.0	0.1	7030
18	0.016127662504249966	0.01617849618730707	25	0.001	100	50	0.0	0.1	7030
19	0.016113685471799533	0.016144044018825996	25	0.001	100	50	0.0	0.1	7030
20	0.01609941961942859	0.016114315290509742	25	0.001	100	50	0.0	0.1	7030
21	0.016090718316741214	0.016135937801382397	25	0.001	100	50	0.0	0.1	7030
22	0.016089411944738233	0.016135536399572006	25	0.001	100	50	0.0	0.1	7030
23	0.016065096178097107	0.0160948045152893	25	0.001	100	50	0.0	0.1	7030
24	0.016054560019449944	0.016109558710931705	25	0.001	100	50	0.0	0.1	7030
25	0.016052299950671166	0.01611246438216183	25	0.001	100	50	0.0	0.1	7030
26	0.016041118842384888	0.016063212055577602	25	0.001	100	50	0.0	0.1	7030
27	0.016021718778144373	0.016055174471087483	25	0.001	100	50	0.0	0.1	7030
28	0.016013400835551708	0.016055796856334858	25	0.001	100	50	0.0	0.1	7030
29	0.01600169799181071	0.016005751037574862	25	0.001	100	50	0.0	0.1	7030
30	0.01599733743899328	0.016033512463590156	25	0.001	100	50	0.0	0.1	7030
31	0.015985673131506496	0.016039980256691375	25	0.001	100	50	0.0	0.1	7030
32	0.015987528081990054	0.016017418055586787	25	0.001	100	50	0.0	0.1	7030
33	0.015973148709579606	0.015990635339192063	25	0.001	100	50	0.0	0.1	7030
34	0.015961487667213003	0.01596500407583855	25	0.001	100	50	0.0	0.1	7030
35	0.015955424513859767	0.016002002214909626	25	0.001	100	50	0.0	0.1	7030
36	0.01595086959947261	0.015955195740848267	25	0.001	100	50	0.0	0.1	7030
37	0.015934681421915353	0.01595534924524578	25	0.001	100	50	0.0	0.1	7030
38	0.015927705670290688	0.015967463217960602	25	0.001	100	50	0.0	0.1	7030
39	0.01590739259618819	0.015962977649690315	25	0.001	100	50	0.0	0.1	7030
40	0.015905379535566824	0.01591588605736626	25	0.001	100	50	0.0	0.1	7030
41	0.015894342842237275	0.015935394845290347	25	0.001	100	50	0.0	0.1	7030
42	0.015893353869974088	0.015937019832232828	25	0.001	100	50	0.0	0.1	7030
43	0.015904724202368638	0.015942872704913125	25	0.001	100	50	0.0	0.1	7030
44	0.01588729544585571	0.01590287578372324	25	0.001	100	50	0.0	0.1	7030
45	0.015880861766018987	0.015895583097026978	25	0.001	100	50	0.0	0.1	7030
46	0.015871305955427684	0.015896687394517328	25	0.001	100	50	0.0	0.1	7030
47	0.015859550587191004	0.01587446274353373	25	0.001	100	50	0.0	0.1	7030
48	0.01585936286904905	0.01589694264457057	25	0.001	100	50	0.0	0.1	7030
49	0.015856048479497363	0.015859854893748207	25	0.001	100	50	0.0	0.1	7030
