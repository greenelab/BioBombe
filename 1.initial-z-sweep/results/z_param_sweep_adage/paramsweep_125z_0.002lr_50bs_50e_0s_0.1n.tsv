	loss	val_loss	num_components	learning_rate	batch_size	epochs	sparsity	noise	seed
0	0.02168367448485782	0.014591957289319534	125	0.002	50	50	0.0	0.1	293
1	0.013368359440695235	0.01253339192379501	125	0.002	50	50	0.0	0.1	293
2	0.012036023029959994	0.011663222897363325	125	0.002	50	50	0.0	0.1	293
3	0.011448726086402078	0.011177709799822612	125	0.002	50	50	0.0	0.1	293
4	0.011188457158002256	0.011066371605270338	125	0.002	50	50	0.0	0.1	293
5	0.01108349066492438	0.010925197838511796	125	0.002	50	50	0.0	0.1	293
6	0.011038490931421256	0.010896130870977848	125	0.002	50	50	0.0	0.1	293
7	0.011012293063468253	0.010908148019978808	125	0.002	50	50	0.0	0.1	293
8	0.011003336297009964	0.010975918290962111	125	0.002	50	50	0.0	0.1	293
9	0.010978417352224231	0.010813214552604788	125	0.002	50	50	0.0	0.1	293
10	0.0109337554469073	0.011365971930438997	125	0.002	50	50	0.0	0.1	293
11	0.01095612667493849	0.01084096661375312	125	0.002	50	50	0.0	0.1	293
12	0.010910439447481112	0.010900613643499679	125	0.002	50	50	0.0	0.1	293
13	0.010935499180477553	0.010820490822737349	125	0.002	50	50	0.0	0.1	293
14	0.010908753458186106	0.010791596860391574	125	0.002	50	50	0.0	0.1	293
15	0.010918871884672653	0.010910358078720806	125	0.002	50	50	0.0	0.1	293
16	0.010873984214020157	0.010944232731046116	125	0.002	50	50	0.0	0.1	293
17	0.010913212063153307	0.010971851283630952	125	0.002	50	50	0.0	0.1	293
18	0.010887679705568648	0.010907712543871162	125	0.002	50	50	0.0	0.1	293
19	0.010892331661037332	0.010762951600349313	125	0.002	50	50	0.0	0.1	293
20	0.010890803841738851	0.0109366851611273	125	0.002	50	50	0.0	0.1	293
21	0.010872057134567152	0.010881892798992005	125	0.002	50	50	0.0	0.1	293
22	0.010890303045459005	0.010754445646079275	125	0.002	50	50	0.0	0.1	293
23	0.010862209633211203	0.010799056848593013	125	0.002	50	50	0.0	0.1	293
24	0.010893958893702626	0.01078462775397768	125	0.002	50	50	0.0	0.1	293
25	0.010846272889893942	0.010700242506628625	125	0.002	50	50	0.0	0.1	293
26	0.01087903493801032	0.010821670229701649	125	0.002	50	50	0.0	0.1	293
27	0.010886350558063695	0.010799644497111013	125	0.002	50	50	0.0	0.1	293
28	0.010849109308830353	0.01089386763603179	125	0.002	50	50	0.0	0.1	293
29	0.010855183083396636	0.010715163271895558	125	0.002	50	50	0.0	0.1	293
30	0.010874681860112737	0.011028080897309583	125	0.002	50	50	0.0	0.1	293
31	0.010891178035434832	0.010861435061032184	125	0.002	50	50	0.0	0.1	293
32	0.01084898360897539	0.010741434505220011	125	0.002	50	50	0.0	0.1	293
33	0.010848322404378547	0.010750985325066582	125	0.002	50	50	0.0	0.1	293
34	0.010857809346668287	0.010745061458182836	125	0.002	50	50	0.0	0.1	293
35	0.01084966546918496	0.011032896748535605	125	0.002	50	50	0.0	0.1	293
36	0.01084516482766289	0.010791794758425389	125	0.002	50	50	0.0	0.1	293
37	0.010860747300344859	0.010834097292409573	125	0.002	50	50	0.0	0.1	293
38	0.010846089908308409	0.01084425198323811	125	0.002	50	50	0.0	0.1	293
39	0.010890440394810822	0.011036319209415871	125	0.002	50	50	0.0	0.1	293
40	0.010862945677437704	0.010693356410794344	125	0.002	50	50	0.0	0.1	293
41	0.010853159604982945	0.010825384463438226	125	0.002	50	50	0.0	0.1	293
42	0.010839126632527795	0.01092197544281558	125	0.002	50	50	0.0	0.1	293
43	0.010845521226498395	0.010850290888938234	125	0.002	50	50	0.0	0.1	293
44	0.010861382687726151	0.010808036425800898	125	0.002	50	50	0.0	0.1	293
45	0.010840699438476245	0.010872243367577146	125	0.002	50	50	0.0	0.1	293
46	0.010833277345486487	0.01075614715283505	125	0.002	50	50	0.0	0.1	293
47	0.010854309948546517	0.010814177452891096	125	0.002	50	50	0.0	0.1	293
48	0.01082917582491031	0.010818144292294636	125	0.002	50	50	0.0	0.1	293
49	0.010845756243123242	0.010852133271143724	125	0.002	50	50	0.0	0.1	293
