	loss	val_loss	num_components	learning_rate	batch_size	epochs	sparsity	noise	seed
0	0.0251518385367076	0.015854283171860824	125	0.001	50	50	0.0	0.5	1432
1	0.01476437970748553	0.0132551487610029	125	0.001	50	50	0.0	0.5	1432
2	0.01317277599009747	0.012162610372069005	125	0.001	50	50	0.0	0.5	1432
3	0.01236259094208422	0.011540408025408452	125	0.001	50	50	0.0	0.5	1432
4	0.01187588832837121	0.011096219209879353	125	0.001	50	50	0.0	0.5	1432
5	0.011533465884680526	0.010890581703835646	125	0.001	50	50	0.0	0.5	1432
6	0.01132573965316743	0.010703681706384992	125	0.001	50	50	0.0	0.5	1432
7	0.011169905465899233	0.010558272528517315	125	0.001	50	50	0.0	0.5	1432
8	0.011064133136688041	0.010555275765748029	125	0.001	50	50	0.0	0.5	1432
9	0.010994725908641227	0.010392179710872428	125	0.001	50	50	0.0	0.5	1432
10	0.010930620224110114	0.010346561251831442	125	0.001	50	50	0.0	0.5	1432
11	0.010894206100432693	0.010363629169719397	125	0.001	50	50	0.0	0.5	1432
12	0.010867892473427133	0.010271261853965473	125	0.001	50	50	0.0	0.5	1432
13	0.010836662115737151	0.010199841318463276	125	0.001	50	50	0.0	0.5	1432
14	0.010815632730306089	0.010252973796660882	125	0.001	50	50	0.0	0.5	1432
15	0.01081373915561062	0.01026847557009805	125	0.001	50	50	0.0	0.5	1432
16	0.010770214416627281	0.010241702250623453	125	0.001	50	50	0.0	0.5	1432
17	0.010768620417821702	0.010235027064628059	125	0.001	50	50	0.0	0.5	1432
18	0.010758804154210207	0.010182432333509621	125	0.001	50	50	0.0	0.5	1432
19	0.010745885177420167	0.010166075472394093	125	0.001	50	50	0.0	0.5	1432
20	0.010743530474869919	0.01012096560095396	125	0.001	50	50	0.0	0.5	1432
21	0.010726951895259709	0.010168621261349486	125	0.001	50	50	0.0	0.5	1432
22	0.010718636391440032	0.010166603067522978	125	0.001	50	50	0.0	0.5	1432
23	0.01071166658739785	0.010193267363492207	125	0.001	50	50	0.0	0.5	1432
24	0.0106917488398908	0.010171791080948157	125	0.001	50	50	0.0	0.5	1432
25	0.01068253301012409	0.010166738200113604	125	0.001	50	50	0.0	0.5	1432
26	0.010672540029315548	0.010150754906194388	125	0.001	50	50	0.0	0.5	1432
27	0.010671398377238527	0.010172447415841583	125	0.001	50	50	0.0	0.5	1432
28	0.010673993415133228	0.01016181138616561	125	0.001	50	50	0.0	0.5	1432
29	0.010666824514547937	0.010127839327827239	125	0.001	50	50	0.0	0.5	1432
30	0.01065692706707369	0.01012327229465228	125	0.001	50	50	0.0	0.5	1432
31	0.01065424099979309	0.01009925167444901	125	0.001	50	50	0.0	0.5	1432
32	0.010650336563616475	0.010232244411672165	125	0.001	50	50	0.0	0.5	1432
33	0.010636768420560794	0.010174637167121553	125	0.001	50	50	0.0	0.5	1432
34	0.010636218402700548	0.010112314956907332	125	0.001	50	50	0.0	0.5	1432
35	0.01062215904531191	0.010262000116289915	125	0.001	50	50	0.0	0.5	1432
36	0.010616588420710027	0.010187462495771694	125	0.001	50	50	0.0	0.5	1432
37	0.010638579571872718	0.010110646756953537	125	0.001	50	50	0.0	0.5	1432
38	0.01062155923221692	0.010128907460441552	125	0.001	50	50	0.0	0.5	1432
39	0.010610238427424309	0.010154499408805006	125	0.001	50	50	0.0	0.5	1432
40	0.010607862516681225	0.010193992322435456	125	0.001	50	50	0.0	0.5	1432
41	0.010598063338629076	0.010119664479210887	125	0.001	50	50	0.0	0.5	1432
42	0.010593454887886477	0.010110693737992707	125	0.001	50	50	0.0	0.5	1432
43	0.010607662114921139	0.010176945735930143	125	0.001	50	50	0.0	0.5	1432
44	0.010592963237524773	0.010100898007019984	125	0.001	50	50	0.0	0.5	1432
45	0.0105908536624797	0.010118156956441087	125	0.001	50	50	0.0	0.5	1432
46	0.010579409772916262	0.010084450495986707	125	0.001	50	50	0.0	0.5	1432
47	0.010586021415407227	0.010181016618133048	125	0.001	50	50	0.0	0.5	1432
48	0.010587517202472752	0.010132847252314336	125	0.001	50	50	0.0	0.5	1432
49	0.010582769828143395	0.010161325483303339	125	0.001	50	50	0.0	0.5	1432
