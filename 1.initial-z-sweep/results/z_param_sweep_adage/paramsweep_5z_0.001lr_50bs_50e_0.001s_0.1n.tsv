	loss	val_loss	num_components	learning_rate	batch_size	epochs	sparsity	noise	seed
0	0.13021782183307118	0.1150530622007742	5	0.001	50	50	0.001	0.1	5305
1	0.10525090123836335	0.09301676562707693	5	0.001	50	50	0.001	0.1	5305
2	0.09483006544127026	0.07635865504381534	5	0.001	50	50	0.001	0.1	5305
3	0.08675534848048991	0.10232223356659052	5	0.001	50	50	0.001	0.1	5305
4	0.08245082736224281	0.10249702196504144	5	0.001	50	50	0.001	0.1	5305
5	0.07629449531819062	0.08093339173447334	5	0.001	50	50	0.001	0.1	5305
6	0.07367170610789929	0.07181720452714377	5	0.001	50	50	0.001	0.1	5305
7	0.07035949047147107	0.07066172711821861	5	0.001	50	50	0.001	0.1	5305
8	0.06688281242190228	0.08556618906172922	5	0.001	50	50	0.001	0.1	5305
9	0.06952850932201365	0.08884100992068734	5	0.001	50	50	0.001	0.1	5305
10	0.06877637912974302	0.06789145473369448	5	0.001	50	50	0.001	0.1	5305
11	0.06319877364530543	0.05977969810395122	5	0.001	50	50	0.001	0.1	5305
12	0.06165844286338538	0.0750168761947187	5	0.001	50	50	0.001	0.1	5305
13	0.05967705053275632	0.0544824962491743	5	0.001	50	50	0.001	0.1	5305
14	0.0638962965820752	0.08266859359141746	5	0.001	50	50	0.001	0.1	5305
15	0.0668943151344842	0.07331649288107743	5	0.001	50	50	0.001	0.1	5305
16	0.06318369268710905	0.07103696595129502	5	0.001	50	50	0.001	0.1	5305
17	0.05922433390697346	0.06442906666212282	5	0.001	50	50	0.001	0.1	5305
18	0.06767651735137427	0.05095749256516278	5	0.001	50	50	0.001	0.1	5305
19	0.05555474294158993	0.08504095489789605	5	0.001	50	50	0.001	0.1	5305
20	0.06380869796493475	0.06879233373738612	5	0.001	50	50	0.001	0.1	5305
21	0.05967741879944934	0.06545873809726926	5	0.001	50	50	0.001	0.1	5305
22	0.0656384692754817	0.06835754272700724	5	0.001	50	50	0.001	0.1	5305
23	0.05951251966023336	0.05795197026451505	5	0.001	50	50	0.001	0.1	5305
24	0.06024380495640571	0.058969778957888914	5	0.001	50	50	0.001	0.1	5305
25	0.06306594283118469	0.07290014039091125	5	0.001	50	50	0.001	0.1	5305
26	0.061656081298453055	0.06616696968931085	5	0.001	50	50	0.001	0.1	5305
27	0.06414516051463616	0.09246177565356285	5	0.001	50	50	0.001	0.1	5305
28	0.06250438692132687	0.08097291029776031	5	0.001	50	50	0.001	0.1	5305
29	0.06156111581388156	0.07216778215213214	5	0.001	50	50	0.001	0.1	5305
30	0.06565812798619625	0.07655234479596465	5	0.001	50	50	0.001	0.1	5305
31	0.06171378908540571	0.08991785357433797	5	0.001	50	50	0.001	0.1	5305
32	0.06464336829711752	0.08541264809511359	5	0.001	50	50	0.001	0.1	5305
33	0.061195895309296526	0.10698577120586746	5	0.001	50	50	0.001	0.1	5305
34	0.06430880304809447	0.06423641715633253	5	0.001	50	50	0.001	0.1	5305
35	0.059676272194829794	0.08598529885079619	5	0.001	50	50	0.001	0.1	5305
36	0.06278250011412925	0.08099481801059004	5	0.001	50	50	0.001	0.1	5305
37	0.061258790702099476	0.05897473608700997	5	0.001	50	50	0.001	0.1	5305
38	0.06385901538817622	0.0562698371907835	5	0.001	50	50	0.001	0.1	5305
39	0.06353840015929133	0.0865092100450915	5	0.001	50	50	0.001	0.1	5305
40	0.060703140671104654	0.08980312239713469	5	0.001	50	50	0.001	0.1	5305
41	0.0645921522786871	0.05519638176345005	5	0.001	50	50	0.001	0.1	5305
42	0.05804435611420226	0.060068261697460996	5	0.001	50	50	0.001	0.1	5305
43	0.061986350808100686	0.08768662563872383	5	0.001	50	50	0.001	0.1	5305
44	0.061209643066315676	0.06621840225806884	5	0.001	50	50	0.001	0.1	5305
45	0.061902204946571567	0.06090440356081804	5	0.001	50	50	0.001	0.1	5305
46	0.06517556409550299	0.10620483341511075	5	0.001	50	50	0.001	0.1	5305
47	0.06497313460525299	0.06762260976416659	5	0.001	50	50	0.001	0.1	5305
48	0.0617356979225136	0.09252541503703389	5	0.001	50	50	0.001	0.1	5305
49	0.0707676157661627	0.09383316845214618	5	0.001	50	50	0.001	0.1	5305
