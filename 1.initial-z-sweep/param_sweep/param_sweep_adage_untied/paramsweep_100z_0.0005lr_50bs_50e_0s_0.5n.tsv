	loss	val_loss	num_components	learning_rate	batch_size	epochs	sparsity	noise	seed
0	0.0299915556380248	0.019409373801479377	100	0.0005	50	50	0.0	0.5	6695
1	0.017325976593001394	0.015246010369427118	100	0.0005	50	50	0.0	0.5	6695
2	0.014901189949087625	0.013885976137668178	100	0.0005	50	50	0.0	0.5	6695
3	0.01382011785210018	0.012965788115635087	100	0.0005	50	50	0.0	0.5	6695
4	0.013076148412822506	0.012414954143289626	100	0.0005	50	50	0.0	0.5	6695
5	0.01252799094812429	0.011876788683036203	100	0.0005	50	50	0.0	0.5	6695
6	0.012140652457679368	0.011529344440318544	100	0.0005	50	50	0.0	0.5	6695
7	0.011845179345103748	0.011252367495065894	100	0.0005	50	50	0.0	0.5	6695
8	0.011608982913241203	0.011036903864524219	100	0.0005	50	50	0.0	0.5	6695
9	0.011444923001135034	0.010920679659705545	100	0.0005	50	50	0.0	0.5	6695
10	0.01130443293702985	0.010804746031091614	100	0.0005	50	50	0.0	0.5	6695
11	0.01120290054669482	0.010691538096955812	100	0.0005	50	50	0.0	0.5	6695
12	0.011104456908604322	0.010557558893245675	100	0.0005	50	50	0.0	0.5	6695
13	0.011033569061692337	0.010451033647327076	100	0.0005	50	50	0.0	0.5	6695
14	0.010966071759666315	0.010423656825481934	100	0.0005	50	50	0.0	0.5	6695
15	0.01091231624016527	0.010382034262040377	100	0.0005	50	50	0.0	0.5	6695
16	0.01088027814926569	0.010310438778571941	100	0.0005	50	50	0.0	0.5	6695
17	0.010841016098540318	0.01027439934616449	100	0.0005	50	50	0.0	0.5	6695
18	0.010803575452282973	0.010247712844473454	100	0.0005	50	50	0.0	0.5	6695
19	0.010781852814191702	0.010247448690762712	100	0.0005	50	50	0.0	0.5	6695
20	0.010760730861945492	0.010185424950735975	100	0.0005	50	50	0.0	0.5	6695
21	0.010746897873624527	0.01021024593495787	100	0.0005	50	50	0.0	0.5	6695
22	0.010732689462574435	0.010203401898220894	100	0.0005	50	50	0.0	0.5	6695
23	0.0107202267370826	0.010153043789016243	100	0.0005	50	50	0.0	0.5	6695
24	0.010698810919567595	0.01013849885781682	100	0.0005	50	50	0.0	0.5	6695
25	0.010696628236442466	0.01011976332227427	100	0.0005	50	50	0.0	0.5	6695
26	0.010680650214253146	0.01014014379785576	100	0.0005	50	50	0.0	0.5	6695
27	0.010667614091765922	0.010099693903090507	100	0.0005	50	50	0.0	0.5	6695
28	0.010666263392152561	0.010139060137262764	100	0.0005	50	50	0.0	0.5	6695
29	0.010663242991346838	0.01011897141494954	100	0.0005	50	50	0.0	0.5	6695
30	0.010647025101502625	0.010092694198367246	100	0.0005	50	50	0.0	0.5	6695
31	0.010647065101871515	0.01009997360614871	100	0.0005	50	50	0.0	0.5	6695
32	0.010656156863007664	0.010105282436863984	100	0.0005	50	50	0.0	0.5	6695
33	0.010639749978892889	0.01009905248538486	100	0.0005	50	50	0.0	0.5	6695
34	0.010639561351689867	0.010057541723389813	100	0.0005	50	50	0.0	0.5	6695
35	0.010633303886335859	0.010086602951626136	100	0.0005	50	50	0.0	0.5	6695
36	0.010625596166121666	0.010131775063193665	100	0.0005	50	50	0.0	0.5	6695
37	0.010618135123704098	0.010101919621585319	100	0.0005	50	50	0.0	0.5	6695
38	0.010615951637185862	0.010084303701385485	100	0.0005	50	50	0.0	0.5	6695
39	0.010610303366223336	0.010068415553731056	100	0.0005	50	50	0.0	0.5	6695
40	0.010618379848820142	0.010090625194928843	100	0.0005	50	50	0.0	0.5	6695
41	0.01061411451530133	0.010076007889459056	100	0.0005	50	50	0.0	0.5	6695
42	0.010606843049416446	0.01008392567701823	100	0.0005	50	50	0.0	0.5	6695
43	0.01060042958293048	0.010079148433831864	100	0.0005	50	50	0.0	0.5	6695
44	0.010593088998821498	0.010101898429099735	100	0.0005	50	50	0.0	0.5	6695
45	0.010600969142136166	0.010109366409224146	100	0.0005	50	50	0.0	0.5	6695
46	0.010596193605019786	0.010064516515679387	100	0.0005	50	50	0.0	0.5	6695
47	0.01059212801748649	0.010150348171093154	100	0.0005	50	50	0.0	0.5	6695
48	0.010586245014648546	0.010098878360116572	100	0.0005	50	50	0.0	0.5	6695
49	0.010587506655365795	0.010119642521010759	100	0.0005	50	50	0.0	0.5	6695
