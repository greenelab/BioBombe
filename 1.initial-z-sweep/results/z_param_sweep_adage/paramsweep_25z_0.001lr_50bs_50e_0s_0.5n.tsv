	loss	val_loss	num_components	learning_rate	batch_size	epochs	sparsity	noise	seed
0	0.03229420694060411	0.022305173258053184	25	0.001	50	50	0.0	0.5	7068
1	0.01974525507355482	0.017943266569058026	25	0.001	50	50	0.0	0.5	7068
2	0.017302345226954244	0.016692599568366436	25	0.001	50	50	0.0	0.5	7068
3	0.016654280391198713	0.016469165458420493	25	0.001	50	50	0.0	0.5	7068
4	0.01648966692986447	0.01631226345013478	25	0.001	50	50	0.0	0.5	7068
5	0.016400425420522145	0.016322291354661234	25	0.001	50	50	0.0	0.5	7068
6	0.016372160479888632	0.01629854329144066	25	0.001	50	50	0.0	0.5	7068
7	0.0163470745596409	0.016183672129153406	25	0.001	50	50	0.0	0.5	7068
8	0.016320777910547923	0.016145695426481747	25	0.001	50	50	0.0	0.5	7068
9	0.01629014474097479	0.016250746401229848	25	0.001	50	50	0.0	0.5	7068
10	0.016287473973583735	0.01614879946407807	25	0.001	50	50	0.0	0.5	7068
11	0.01625659529598204	0.01612437870460304	25	0.001	50	50	0.0	0.5	7068
12	0.01623819989559787	0.016150280722838972	25	0.001	50	50	0.0	0.5	7068
13	0.016218525470237125	0.01615181226055378	25	0.001	50	50	0.0	0.5	7068
14	0.01621537261751251	0.016137602669587556	25	0.001	50	50	0.0	0.5	7068
15	0.016196586594439098	0.016068316274901306	25	0.001	50	50	0.0	0.5	7068
16	0.01618479658541537	0.016075463397481816	25	0.001	50	50	0.0	0.5	7068
17	0.01616369907937261	0.01602861170352474	25	0.001	50	50	0.0	0.5	7068
18	0.016153887984388684	0.016126255821757175	25	0.001	50	50	0.0	0.5	7068
19	0.016144203702631962	0.016022238043452083	25	0.001	50	50	0.0	0.5	7068
20	0.016134325795419395	0.01606929758795133	25	0.001	50	50	0.0	0.5	7068
21	0.016130946196071923	0.015980439919472653	25	0.001	50	50	0.0	0.5	7068
22	0.016118099312435033	0.015996453820604894	25	0.001	50	50	0.0	0.5	7068
23	0.01610787571882272	0.016023615749114786	25	0.001	50	50	0.0	0.5	7068
24	0.01608428308077558	0.015999109630275298	25	0.001	50	50	0.0	0.5	7068
25	0.016080155420142506	0.01596494825346748	25	0.001	50	50	0.0	0.5	7068
26	0.016068822033145306	0.015924528331974837	25	0.001	50	50	0.0	0.5	7068
27	0.01606161151748971	0.01591848129367395	25	0.001	50	50	0.0	0.5	7068
28	0.01604579571540736	0.015908904615825267	25	0.001	50	50	0.0	0.5	7068
29	0.016033110157558435	0.015875549171383476	25	0.001	50	50	0.0	0.5	7068
30	0.016029395684868993	0.015841273866935738	25	0.001	50	50	0.0	0.5	7068
31	0.016015119048825462	0.015944273923354554	25	0.001	50	50	0.0	0.5	7068
32	0.01602185646250716	0.01581040274772088	25	0.001	50	50	0.0	0.5	7068
33	0.01600240840898441	0.015914397542124495	25	0.001	50	50	0.0	0.5	7068
34	0.016005942009885685	0.015900094243724534	25	0.001	50	50	0.0	0.5	7068
35	0.015997673122159457	0.015911121843792283	25	0.001	50	50	0.0	0.5	7068
36	0.015990489753572458	0.015848551543846186	25	0.001	50	50	0.0	0.5	7068
37	0.01598618466921153	0.015845082513275724	25	0.001	50	50	0.0	0.5	7068
38	0.015970174438181287	0.015827113194444555	25	0.001	50	50	0.0	0.5	7068
39	0.015973632128426996	0.015904670810935943	25	0.001	50	50	0.0	0.5	7068
40	0.015989753089288742	0.015837200897715516	25	0.001	50	50	0.0	0.5	7068
41	0.015960186969816943	0.015817413446680424	25	0.001	50	50	0.0	0.5	7068
42	0.015950819575290626	0.015826879332757362	25	0.001	50	50	0.0	0.5	7068
43	0.015946442504433906	0.015877277364181513	25	0.001	50	50	0.0	0.5	7068
44	0.015940808372398522	0.015785463818528796	25	0.001	50	50	0.0	0.5	7068
45	0.015936017913046722	0.015828858913202015	25	0.001	50	50	0.0	0.5	7068
46	0.01594446326211106	0.015868662434485856	25	0.001	50	50	0.0	0.5	7068
47	0.01594486421621797	0.015839141535342993	25	0.001	50	50	0.0	0.5	7068
48	0.01593080785077607	0.015754185084531344	25	0.001	50	50	0.0	0.5	7068
49	0.015922795418991453	0.01580247818803981	25	0.001	50	50	0.0	0.5	7068
