	loss	val_loss	num_components	learning_rate	batch_size	epochs	sparsity	noise	seed
0	0.02584054097970541	0.01673217859104931	75	0.001	50	50	0.0	0.0	5395
1	0.015197292055737183	0.013865700636208627	75	0.001	50	50	0.0	0.0	5395
2	0.013221772734111799	0.012473529092540134	75	0.001	50	50	0.0	0.0	5395
3	0.012213154502302443	0.011731765468036809	75	0.001	50	50	0.0	0.0	5395
4	0.01164817542623672	0.01135675827785614	75	0.001	50	50	0.0	0.0	5395
5	0.01137444131415898	0.011243033260249499	75	0.001	50	50	0.0	0.0	5395
6	0.011242734767163356	0.01110161962005422	75	0.001	50	50	0.0	0.0	5395
7	0.011169104034279575	0.011092778756488007	75	0.001	50	50	0.0	0.0	5395
8	0.01112838328772364	0.011032211097459492	75	0.001	50	50	0.0	0.0	5395
9	0.011089774062462261	0.011049535606291508	75	0.001	50	50	0.0	0.0	5395
10	0.011072667112280525	0.01102326967420473	75	0.001	50	50	0.0	0.0	5395
11	0.011043073924595342	0.010969979175674528	75	0.001	50	50	0.0	0.0	5395
12	0.011039250247235077	0.011087411654895851	75	0.001	50	50	0.0	0.0	5395
13	0.011025948617177955	0.010950013165349715	75	0.001	50	50	0.0	0.0	5395
14	0.01100870360339627	0.01092724354463998	75	0.001	50	50	0.0	0.0	5395
15	0.011002914773170282	0.01093613595682166	75	0.001	50	50	0.0	0.0	5395
16	0.010989085949334444	0.010938887076584148	75	0.001	50	50	0.0	0.0	5395
17	0.010975785997399302	0.010906355900700644	75	0.001	50	50	0.0	0.0	5395
18	0.01097335465203721	0.010909484004883192	75	0.001	50	50	0.0	0.0	5395
19	0.010966340240384344	0.0108969298175342	75	0.001	50	50	0.0	0.0	5395
20	0.010955843777172873	0.010922495480519977	75	0.001	50	50	0.0	0.0	5395
21	0.010952199415570806	0.010890200345939021	75	0.001	50	50	0.0	0.0	5395
22	0.010940036482746482	0.0109448859318521	75	0.001	50	50	0.0	0.0	5395
23	0.01094654899236564	0.010893767391532716	75	0.001	50	50	0.0	0.0	5395
24	0.01092660942106084	0.010882729370624226	75	0.001	50	50	0.0	0.0	5395
25	0.010932774272998942	0.01089589378794737	75	0.001	50	50	0.0	0.0	5395
26	0.010925326492998724	0.010856057472712222	75	0.001	50	50	0.0	0.0	5395
27	0.010928659035870642	0.010860131731182053	75	0.001	50	50	0.0	0.0	5395
28	0.01091314173875121	0.01085574600674625	75	0.001	50	50	0.0	0.0	5395
29	0.010915936453472342	0.010886968937960683	75	0.001	50	50	0.0	0.0	5395
30	0.010905642561479164	0.01089923065974881	75	0.001	50	50	0.0	0.0	5395
31	0.010905058555242474	0.01087965471865912	75	0.001	50	50	0.0	0.0	5395
32	0.010905255131980703	0.010910979735648996	75	0.001	50	50	0.0	0.0	5395
33	0.010896376252900993	0.010863924500519984	75	0.001	50	50	0.0	0.0	5395
34	0.01088810826820049	0.010841956344320943	75	0.001	50	50	0.0	0.0	5395
35	0.010904326971034993	0.01086039995030281	75	0.001	50	50	0.0	0.0	5395
36	0.010890235425187078	0.010852314587005807	75	0.001	50	50	0.0	0.0	5395
37	0.010884164330114894	0.010886839772601315	75	0.001	50	50	0.0	0.0	5395
38	0.01088264992421626	0.010844751278981993	75	0.001	50	50	0.0	0.0	5395
39	0.010875153270014114	0.010830489922151056	75	0.001	50	50	0.0	0.0	5395
40	0.010871810159554653	0.010843721262925442	75	0.001	50	50	0.0	0.0	5395
41	0.01086436767284645	0.010817451682119821	75	0.001	50	50	0.0	0.0	5395
42	0.010871200355197841	0.010837472599519712	75	0.001	50	50	0.0	0.0	5395
43	0.010871521409188987	0.010818103397795735	75	0.001	50	50	0.0	0.0	5395
44	0.010865674131224086	0.01083712512003988	75	0.001	50	50	0.0	0.0	5395
45	0.010867954927149233	0.01084757489766605	75	0.001	50	50	0.0	0.0	5395
46	0.010859059237068782	0.01082175004208749	75	0.001	50	50	0.0	0.0	5395
47	0.010849101556086881	0.010819071403439256	75	0.001	50	50	0.0	0.0	5395
48	0.010854023792883338	0.010810484116038674	75	0.001	50	50	0.0	0.0	5395
49	0.010856827917989333	0.010837044363866344	75	0.001	50	50	0.0	0.0	5395
