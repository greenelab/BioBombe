	loss	val_loss	num_components	learning_rate	batch_size	epochs	sparsity	noise	seed
0	0.03188459463789545	0.020954861093747	75	0.001	100	50	0.0	0.1	8667
1	0.017864734812792122	0.016088840129641	75	0.001	100	50	0.0	0.1	8667
2	0.01501803992370148	0.014341636238544895	75	0.001	100	50	0.0	0.1	8667
3	0.013748079635108693	0.013391930597476362	75	0.001	100	50	0.0	0.1	8667
4	0.012955411101739806	0.012733285920783507	75	0.001	100	50	0.0	0.1	8667
5	0.0124242679651007	0.012223252992770753	75	0.001	100	50	0.0	0.1	8667
6	0.01203770366234454	0.011878714994678762	75	0.001	100	50	0.0	0.1	8667
7	0.011739026860168849	0.0116513820924221	75	0.001	100	50	0.0	0.1	8667
8	0.011507849025081071	0.0114393359260281	75	0.001	100	50	0.0	0.1	8667
9	0.011353329665033595	0.011325942215297806	75	0.001	100	50	0.0	0.1	8667
10	0.011216537675889648	0.011260025009128264	75	0.001	100	50	0.0	0.1	8667
11	0.011140030280209134	0.011153760699138586	75	0.001	100	50	0.0	0.1	8667
12	0.011079291917951982	0.011099468047657615	75	0.001	100	50	0.0	0.1	8667
13	0.0110334585020377	0.01105040010869731	75	0.001	100	50	0.0	0.1	8667
14	0.011007824088397821	0.011073081469230967	75	0.001	100	50	0.0	0.1	8667
15	0.010985582870643273	0.011038622462740934	75	0.001	100	50	0.0	0.1	8667
16	0.010974916600998506	0.011013334846733063	75	0.001	100	50	0.0	0.1	8667
17	0.010949329833912568	0.011018205884609009	75	0.001	100	50	0.0	0.1	8667
18	0.010941950109520444	0.011015591665703525	75	0.001	100	50	0.0	0.1	8667
19	0.010933522227132477	0.011014726109104676	75	0.001	100	50	0.0	0.1	8667
20	0.010934184168733662	0.010930939315752362	75	0.001	100	50	0.0	0.1	8667
21	0.010916328770311843	0.010962915314756775	75	0.001	100	50	0.0	0.1	8667
22	0.010908956589602892	0.010931089212387857	75	0.001	100	50	0.0	0.1	8667
23	0.010895330259020595	0.010917554887271634	75	0.001	100	50	0.0	0.1	8667
24	0.010900009276073683	0.010918736635897857	75	0.001	100	50	0.0	0.1	8667
25	0.010899251804502163	0.011080165790753656	75	0.001	100	50	0.0	0.1	8667
26	0.010896601721614324	0.010916809200613721	75	0.001	100	50	0.0	0.1	8667
27	0.010879979811709314	0.010910611624614002	75	0.001	100	50	0.0	0.1	8667
28	0.010870968357233543	0.01090552439168808	75	0.001	100	50	0.0	0.1	8667
29	0.01087086479402473	0.010915539907665485	75	0.001	100	50	0.0	0.1	8667
30	0.010868891405093452	0.010923675124321568	75	0.001	100	50	0.0	0.1	8667
31	0.010873296192715503	0.010901879764965114	75	0.001	100	50	0.0	0.1	8667
32	0.010855686058039292	0.01088111523724024	75	0.001	100	50	0.0	0.1	8667
33	0.01086388109553624	0.010911353418592855	75	0.001	100	50	0.0	0.1	8667
34	0.010853185677068033	0.010910879334445551	75	0.001	100	50	0.0	0.1	8667
35	0.010852653908796674	0.010891127298598535	75	0.001	100	50	0.0	0.1	8667
36	0.010846046592656736	0.01090506266937885	75	0.001	100	50	0.0	0.1	8667
37	0.010852617249039215	0.010948800574453908	75	0.001	100	50	0.0	0.1	8667
38	0.010866182315752305	0.010877353623580636	75	0.001	100	50	0.0	0.1	8667
39	0.010837911285557352	0.010873804864318718	75	0.001	100	50	0.0	0.1	8667
40	0.010837107666670282	0.010875897425054137	75	0.001	100	50	0.0	0.1	8667
41	0.010836791294917478	0.010891492004874902	75	0.001	100	50	0.0	0.1	8667
42	0.010831504127587127	0.010928309113000934	75	0.001	100	50	0.0	0.1	8667
43	0.010840525846003467	0.010876770818652319	75	0.001	100	50	0.0	0.1	8667
44	0.010827781562987764	0.010936747823288063	75	0.001	100	50	0.0	0.1	8667
45	0.01083256822680184	0.010915930676728088	75	0.001	100	50	0.0	0.1	8667
46	0.010832036873485	0.010870567120497701	75	0.001	100	50	0.0	0.1	8667
47	0.010826315433891112	0.010866684646810445	75	0.001	100	50	0.0	0.1	8667
48	0.010827987762337237	0.010886985204942938	75	0.001	100	50	0.0	0.1	8667
49	0.010830439075283032	0.010880591638072841	75	0.001	100	50	0.0	0.1	8667
