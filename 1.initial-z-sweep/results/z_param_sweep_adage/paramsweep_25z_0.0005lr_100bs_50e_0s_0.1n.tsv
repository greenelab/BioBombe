	loss	val_loss	num_components	learning_rate	batch_size	epochs	sparsity	noise	seed
0	0.04277689700130352	0.030441239935632646	25	0.0005	100	50	0.0	0.1	7059
1	0.026877309653492863	0.024115258884788927	25	0.0005	100	50	0.0	0.1	7059
2	0.02259669103983718	0.02113226211378725	25	0.0005	100	50	0.0	0.1	7059
3	0.020159113072955138	0.01925030074671737	25	0.0005	100	50	0.0	0.1	7059
4	0.01858254913947103	0.018034782627487044	25	0.0005	100	50	0.0	0.1	7059
5	0.01753431165292677	0.01711577948104469	25	0.0005	100	50	0.0	0.1	7059
6	0.016754027096497055	0.016495940045775922	25	0.0005	100	50	0.0	0.1	7059
7	0.016273601338109813	0.01608815756813404	25	0.0005	100	50	0.0	0.1	7059
8	0.015950663753141192	0.015792176483580077	25	0.0005	100	50	0.0	0.1	7059
9	0.01569594406256365	0.015553403018936372	25	0.0005	100	50	0.0	0.1	7059
10	0.01553132559258788	0.015415400715391677	25	0.0005	100	50	0.0	0.1	7059
11	0.015439602814026673	0.015360472207429532	25	0.0005	100	50	0.0	0.1	7059
12	0.015373177649697664	0.015291166913839872	25	0.0005	100	50	0.0	0.1	7059
13	0.015330226227568286	0.015232630626050965	25	0.0005	100	50	0.0	0.1	7059
14	0.015293161316195887	0.015190940161536574	25	0.0005	100	50	0.0	0.1	7059
15	0.015255937065652538	0.015147291935880599	25	0.0005	100	50	0.0	0.1	7059
16	0.01522973043814224	0.0151278783618891	25	0.0005	100	50	0.0	0.1	7059
17	0.015210459966552469	0.015112504811804336	25	0.0005	100	50	0.0	0.1	7059
18	0.015190775993972791	0.015072119573619125	25	0.0005	100	50	0.0	0.1	7059
19	0.015172538815132903	0.015072561358858478	25	0.0005	100	50	0.0	0.1	7059
20	0.01515824451809477	0.015069438386744113	25	0.0005	100	50	0.0	0.1	7059
21	0.015147936662496194	0.015050608682558368	25	0.0005	100	50	0.0	0.1	7059
22	0.015136884175170548	0.015043620885586648	25	0.0005	100	50	0.0	0.1	7059
23	0.015129677871689177	0.015016076320614915	25	0.0005	100	50	0.0	0.1	7059
24	0.015109664841166126	0.014991579705539443	25	0.0005	100	50	0.0	0.1	7059
25	0.015098066572691585	0.014986611916959628	25	0.0005	100	50	0.0	0.1	7059
26	0.015100501957676477	0.014977290991330466	25	0.0005	100	50	0.0	0.1	7059
27	0.015091682094845137	0.014988779234584157	25	0.0005	100	50	0.0	0.1	7059
28	0.015080397108621784	0.014969081363247184	25	0.0005	100	50	0.0	0.1	7059
29	0.015076837776879614	0.014956845362344845	25	0.0005	100	50	0.0	0.1	7059
30	0.015063166366643711	0.014956188924168994	25	0.0005	100	50	0.0	0.1	7059
31	0.015060273437709024	0.014956035904130447	25	0.0005	100	50	0.0	0.1	7059
32	0.015049819639618479	0.014949570135720944	25	0.0005	100	50	0.0	0.1	7059
33	0.015054213435200225	0.01492636665046557	25	0.0005	100	50	0.0	0.1	7059
34	0.015042979772241553	0.014917707047558765	25	0.0005	100	50	0.0	0.1	7059
35	0.015032017937142386	0.014910586673346122	25	0.0005	100	50	0.0	0.1	7059
36	0.01503086642351393	0.014924132608831726	25	0.0005	100	50	0.0	0.1	7059
37	0.01502649643706073	0.014917112283237692	25	0.0005	100	50	0.0	0.1	7059
38	0.015018918399433773	0.014909317720517602	25	0.0005	100	50	0.0	0.1	7059
39	0.015016891152196168	0.014892826629642319	25	0.0005	100	50	0.0	0.1	7059
40	0.015013589075138532	0.014890415704387096	25	0.0005	100	50	0.0	0.1	7059
41	0.015005051018467374	0.014924757715036832	25	0.0005	100	50	0.0	0.1	7059
42	0.014999994813199125	0.014936655467771435	25	0.0005	100	50	0.0	0.1	7059
43	0.01500452819051676	0.014873723976712952	25	0.0005	100	50	0.0	0.1	7059
44	0.014993879980792829	0.014874814906251931	25	0.0005	100	50	0.0	0.1	7059
45	0.014987261904330154	0.01487226210299516	25	0.0005	100	50	0.0	0.1	7059
46	0.014985997653376831	0.014869626930222115	25	0.0005	100	50	0.0	0.1	7059
47	0.01498318303182118	0.01486135786666355	25	0.0005	100	50	0.0	0.1	7059
48	0.014972822432970502	0.014877208738265485	25	0.0005	100	50	0.0	0.1	7059
49	0.01497777308643412	0.01485093471275404	25	0.0005	100	50	0.0	0.1	7059
