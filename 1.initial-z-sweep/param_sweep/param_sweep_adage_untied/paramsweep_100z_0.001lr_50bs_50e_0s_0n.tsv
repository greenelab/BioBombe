	loss	val_loss	num_components	learning_rate	batch_size	epochs	sparsity	noise	seed
0	0.025405602764528617	0.01687191810679709	100	0.001	50	50	0.0	0.0	4873
1	0.014908435488032456	0.013981567387670335	100	0.001	50	50	0.0	0.0	4873
2	0.012960254820513887	0.012897577676316635	100	0.001	50	50	0.0	0.0	4873
3	0.011998300968058796	0.012010829628268915	100	0.001	50	50	0.0	0.0	4873
4	0.011401906541959172	0.011564536935455713	100	0.001	50	50	0.0	0.0	4873
5	0.011025543609398763	0.011219203326117469	100	0.001	50	50	0.0	0.0	4873
6	0.010798155476552773	0.011068875901401043	100	0.001	50	50	0.0	0.0	4873
7	0.01068403527291402	0.010982670012797385	100	0.001	50	50	0.0	0.0	4873
8	0.010612352446756936	0.010941689426211166	100	0.001	50	50	0.0	0.0	4873
9	0.010567757481795993	0.010878091731445284	100	0.001	50	50	0.0	0.0	4873
10	0.010542552505813648	0.010875535315084298	100	0.001	50	50	0.0	0.0	4873
11	0.010518638642708035	0.010835383862014825	100	0.001	50	50	0.0	0.0	4873
12	0.010493775119184402	0.010839510577273186	100	0.001	50	50	0.0	0.0	4873
13	0.010473734283047865	0.010812141587085182	100	0.001	50	50	0.0	0.0	4873
14	0.01047889237686585	0.010806210705900056	100	0.001	50	50	0.0	0.0	4873
15	0.010450365121556395	0.010815005144015325	100	0.001	50	50	0.0	0.0	4873
16	0.010439777835347941	0.01081266624400709	100	0.001	50	50	0.0	0.0	4873
17	0.010434920651626253	0.010790988439640626	100	0.001	50	50	0.0	0.0	4873
18	0.010422386476981974	0.010780300006598064	100	0.001	50	50	0.0	0.0	4873
19	0.010414855232847955	0.010834126779542484	100	0.001	50	50	0.0	0.0	4873
20	0.010406698212071084	0.01077559987086639	100	0.001	50	50	0.0	0.0	4873
21	0.010400414356737648	0.010776299564791113	100	0.001	50	50	0.0	0.0	4873
22	0.010395399402440367	0.010746057188035084	100	0.001	50	50	0.0	0.0	4873
23	0.010392424376327439	0.010766838191087797	100	0.001	50	50	0.0	0.0	4873
24	0.010384283795427227	0.01076061069345668	100	0.001	50	50	0.0	0.0	4873
25	0.010377286671369263	0.010767490122232224	100	0.001	50	50	0.0	0.0	4873
26	0.010374879341128875	0.010753851779246877	100	0.001	50	50	0.0	0.0	4873
27	0.01037650554897326	0.01074203661149706	100	0.001	50	50	0.0	0.0	4873
28	0.010362179768126797	0.010722837184714314	100	0.001	50	50	0.0	0.0	4873
29	0.010356627193452808	0.01072529072821653	100	0.001	50	50	0.0	0.0	4873
30	0.010353902987854826	0.010728151138594114	100	0.001	50	50	0.0	0.0	4873
31	0.01034722398322649	0.010705952208496303	100	0.001	50	50	0.0	0.0	4873
32	0.010342914774649032	0.010703631952746876	100	0.001	50	50	0.0	0.0	4873
33	0.010342594194976427	0.010728582253690317	100	0.001	50	50	0.0	0.0	4873
34	0.010338459421445107	0.010743488149849224	100	0.001	50	50	0.0	0.0	4873
35	0.010336331435341005	0.01071046589807844	100	0.001	50	50	0.0	0.0	4873
36	0.010341844796021522	0.010714788534758652	100	0.001	50	50	0.0	0.0	4873
37	0.010332598581076663	0.010782110880540388	100	0.001	50	50	0.0	0.0	4873
38	0.010327974105906622	0.010707408572630831	100	0.001	50	50	0.0	0.0	4873
39	0.010327920586964624	0.010713453783901199	100	0.001	50	50	0.0	0.0	4873
40	0.010319071994419846	0.010714225533345348	100	0.001	50	50	0.0	0.0	4873
41	0.010320156911805765	0.010696038714188007	100	0.001	50	50	0.0	0.0	4873
42	0.010316723811226558	0.01069770399018068	100	0.001	50	50	0.0	0.0	4873
43	0.010323411986112285	0.010703979473183533	100	0.001	50	50	0.0	0.0	4873
44	0.010312033302485974	0.010691159972867592	100	0.001	50	50	0.0	0.0	4873
45	0.010310938808495929	0.010696669928302158	100	0.001	50	50	0.0	0.0	4873
46	0.010302910860340964	0.010756546184491815	100	0.001	50	50	0.0	0.0	4873
47	0.010307474613437444	0.010679310574116146	100	0.001	50	50	0.0	0.0	4873
48	0.010304701878344622	0.01071635927349671	100	0.001	50	50	0.0	0.0	4873
49	0.010314036600298062	0.010697241468323007	100	0.001	50	50	0.0	0.0	4873
