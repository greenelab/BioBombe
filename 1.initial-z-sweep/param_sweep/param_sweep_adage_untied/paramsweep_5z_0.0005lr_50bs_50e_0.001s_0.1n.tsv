	loss	val_loss	num_components	learning_rate	batch_size	epochs	sparsity	noise	seed
0	0.12992750345539822	0.10404174138095593	5	0.0005	50	50	0.001	0.1	2730
1	0.11393674649199251	0.09332420547309386	5	0.0005	50	50	0.001	0.1	2730
2	0.10367439194134898	0.09093856529455568	5	0.0005	50	50	0.001	0.1	2730
3	0.09572726696916459	0.08886322443218578	5	0.0005	50	50	0.001	0.1	2730
4	0.08921648484148052	0.08300601334861314	5	0.0005	50	50	0.001	0.1	2730
5	0.08407513210766629	0.08371544726207197	5	0.0005	50	50	0.001	0.1	2730
6	0.0792257920695924	0.07926591521353385	5	0.0005	50	50	0.001	0.1	2730
7	0.07570533068235427	0.07097747514512297	5	0.0005	50	50	0.001	0.1	2730
8	0.07185385687836524	0.07586761854631723	5	0.0005	50	50	0.001	0.1	2730
9	0.06903490934488904	0.06301983379639585	5	0.0005	50	50	0.001	0.1	2730
10	0.06502896290390014	0.06472432532171446	5	0.0005	50	50	0.001	0.1	2730
11	0.06335875686049512	0.0640166250857067	5	0.0005	50	50	0.001	0.1	2730
12	0.06013698523637973	0.06802991458551377	5	0.0005	50	50	0.001	0.1	2730
13	0.05872892111565169	0.06311020361932697	5	0.0005	50	50	0.001	0.1	2730
14	0.056581831532932676	0.0701911107531925	5	0.0005	50	50	0.001	0.1	2730
15	0.055818165462587964	0.05545072605117557	5	0.0005	50	50	0.001	0.1	2730
16	0.054216084808101694	0.05516933498060042	5	0.0005	50	50	0.001	0.1	2730
17	0.05344029872640722	0.060064235349572184	5	0.0005	50	50	0.001	0.1	2730
18	0.05152536316214884	0.052024957326812456	5	0.0005	50	50	0.001	0.1	2730
19	0.052848973374310784	0.05008043843832572	5	0.0005	50	50	0.001	0.1	2730
20	0.051562000516416996	0.0541983791778352	5	0.0005	50	50	0.001	0.1	2730
21	0.04901700132849694	0.0533639581920312	5	0.0005	50	50	0.001	0.1	2730
22	0.04912336412166214	0.05886119149668039	5	0.0005	50	50	0.001	0.1	2730
23	0.05191715945040814	0.05027876067799774	5	0.0005	50	50	0.001	0.1	2730
24	0.04950141878678887	0.053321019085711775	5	0.0005	50	50	0.001	0.1	2730
25	0.048069086369464616	0.05580421474479125	5	0.0005	50	50	0.001	0.1	2730
26	0.04845697797093219	0.05933720674045227	5	0.0005	50	50	0.001	0.1	2730
27	0.05133308200340753	0.047182297480094955	5	0.0005	50	50	0.001	0.1	2730
28	0.04676381525961813	0.056748247176522736	5	0.0005	50	50	0.001	0.1	2730
29	0.051197992124650454	0.0470236770979647	5	0.0005	50	50	0.001	0.1	2730
30	0.04811111340511178	0.04786285987376711	5	0.0005	50	50	0.001	0.1	2730
31	0.04570384091483626	0.052099257315834896	5	0.0005	50	50	0.001	0.1	2730
32	0.04979728230636444	0.055028505658955236	5	0.0005	50	50	0.001	0.1	2730
33	0.04890535640889737	0.060243700176876316	5	0.0005	50	50	0.001	0.1	2730
34	0.049766349160293204	0.053971595328522005	5	0.0005	50	50	0.001	0.1	2730
35	0.04846533947583404	0.06074946674288573	5	0.0005	50	50	0.001	0.1	2730
36	0.05133214102032764	0.05186729200796917	5	0.0005	50	50	0.001	0.1	2730
37	0.04659594203233972	0.0476441705959135	5	0.0005	50	50	0.001	0.1	2730
38	0.04998988525570547	0.05821675530155804	5	0.0005	50	50	0.001	0.1	2730
39	0.047476139907805	0.07281957073831649	5	0.0005	50	50	0.001	0.1	2730
40	0.04841194180622309	0.04614414366578281	5	0.0005	50	50	0.001	0.1	2730
41	0.04667826647307571	0.060754866383784585	5	0.0005	50	50	0.001	0.1	2730
42	0.05044406896674151	0.0441118780003565	5	0.0005	50	50	0.001	0.1	2730
43	0.04877376309560878	0.043231313346562834	5	0.0005	50	50	0.001	0.1	2730
44	0.04709302438416112	0.05145901347551474	5	0.0005	50	50	0.001	0.1	2730
45	0.04903098794220168	0.05284592871826648	5	0.0005	50	50	0.001	0.1	2730
46	0.0487066871470477	0.049310794253308275	5	0.0005	50	50	0.001	0.1	2730
47	0.04795324096679181	0.04932522865628421	5	0.0005	50	50	0.001	0.1	2730
48	0.04687717201877423	0.07060383629217658	5	0.0005	50	50	0.001	0.1	2730
49	0.05044162860635894	0.05938247326996541	5	0.0005	50	50	0.001	0.1	2730
